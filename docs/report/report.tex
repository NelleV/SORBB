\documentclass{article}
\usepackage{graphicx,amssymb,amsmath,amsbsy,MnSymbol}

\usepackage[T1]{fontenc}        % pour les charactères accentués
\usepackage[utf8]{inputenc}

\DeclareMathOperator{\score}{score}
\DeclareMathOperator{\tfidf}{tf-idf}


\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.3in}
\setlength{\topmargin}{-0.4in}
\setlength{\topskip}{1in}    % between header and text
\setlength{\textheight}{8in} % height of main text
\setlength{\textwidth}{6in}    % width of text
\setlength{\oddsidemargin}{0.5in} % odd page left margin
\setlength{\evensidemargin}{0.5in} % even page left margin

\bibliographystyle{plain}

\pagenumbering{arabic}
\date{\today}
\title{Smooth object retrieval using Bag of Boundaries}
\author{Vadim Kantorov \& Nelle Varoquaux}
\begin{document}
\maketitle
\begin{abstract}
\textit{Image retrieval systems have been one of the very active research
field of vision. Yet, standard methods fail to perform on wiry objects and
smooth objects. We will review here the method presented in
\cite{Arandjelovic11}.\\
It introduces a new boundary descriptors, representing a 3D object's shape,
tolerant to lighting changes, scale changes and small viewpoints changes. In
order to extract these descriptors on a image, a gPb segmentation of the images
is first computed as described in \cite{gpb}.\\
Finally, we will show the performance of this new technique using the
Sculptures 6k dataset, and the groundtruth provided for $70$ queries.}
\end{abstract}

\section{Introduction}
Object recognition is one of the many very fields of vision. Extracting
viewpoints and lightning invariant descriptors is now done efficiently,
allowing performant commercial applications. \\
However, current methods fail on two type of objects: smooth objects and wiry
objects. We will focus on the smooth objects, using methods
\cite{Arandjelovic11} introduces: boundary descriptors. These new descriptors
focus on describing the form of the objects, allowing to retrieve objects
of same shape, but of different sizes and materials. \\
We will use the sculptures 6K dataset: it contains $6000$ pictures of
sculptures, mostly the work of Moore and Rodin, with groundtruth for twenty
of this sculptures. Most sculptures appear several times in the dataset, taken
from different points of views. As Henry Moore often made sculptures of the
same form in different materials, such as bronze and marble, this dataset is
pertinent to test shape and boundaries descriptors.

\section{Segmentation}

The purpose of the segmentation stage is to find the sculpture region
on an image. Our approach is similar to the one described in {[}???{]}.
The high-level steps are: first, segmentation into super-pixels using
a general-purpose segmentation engine; second, the segmented regions
are classified as foreground and background. Below we present the
details and classification results.
\begin{enumerate}
\item \textbf{Data set. }We use a random subset of 300 images drawn from
the Sculptures 6K train set. The ground truth data for regions (is
a region background or foreground) is extracted from the masks provided
with Sculptures 6K. The train/test separation is half-half (more fine-grained
statistics is found in Table 1).
\item \textbf{Segmentation. }We use the state-of-the-art gPb segmentor from
{[}???{]}. The probability threshold of 0.1 yields about 70-150 regions
per image. It's worth noting that we have to downsample the input
images in order to be able to run gPb. Even then it takes up to 2
minutes to segment a single image.
\item \textbf{Classification. }We construct the feature vector similarly
to {[}???{]} with minor deviations:

\begin{enumerate}
\item mean gradient magnitude (on the grayscale image variant)
\item four bits, whether a region touches a certain image boundary
\item mean channel values for the HSV representation
\item HSV color code histogram (with a dictionary of 1600 words)
\item histogram of dense SIFT descriptors (with a 1000 words dictionary)
\end{enumerate}

This makes up a vector of 2608 elements to describe a region. Quantization
is done using parameters similar to Assignment 2. We use SVM with
different kernels to solve the classification problem. The classification
results are presented in the Table 2. The performance is not stunning
which might be calling for better vocabulary building (larger dictionary
size, using more descriptors from an image).

\end{enumerate}
\begin{table}
\caption{Super-pixel data set statistics}


\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & Images & Foreground regions & Background regions & Total (regions)\tabularnewline
\hline 
\hline 
Train & 150 & 41.66\% & 58.34\% & 9421\tabularnewline
\hline 
Test & 150 & 38.24\% & 61.76\% & 8863\tabularnewline
\hline 
\end{tabular}
\end{table}


\begin{table}
\begin{centering}
\caption{Classification results}
\begin{tabular}{|c|c|c|}
\hline 
 & Linear kernel & RBF\tabularnewline
\hline 
\hline 
Train & 83.82\% & 87.09\%\tabularnewline
\hline 
Test & 63.52\% & 64.06\%\tabularnewline
\hline 
\end{tabular}
\par\end{centering}

\end{table}


\begin{figure}


\caption{Segmentation examples}


\begin{centering}
\includegraphics[height=3cm]{images/Henry_Moore_Goslar_Warrior_0072}\includegraphics[height=3cm]{images/Henry_Moore_Goslar_Warrior_0072_gpb}\includegraphics[height=3cm]{images/Henry_Moore_Goslar_Warrior_0072_MASK}
\par\end{centering}

\begin{centering}
\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0097}\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0097_gpb}\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0097_MASK}
\par\end{centering}

\centering{}\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0235}\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0235_gpb}\includegraphics[height=3cm]{images/Rodin_Burghers_of_Calais_0235_MASK}
\end{figure}



\section{Boundary descriptors}

\begin{figure}

\label{boundary-descriptors}
\begin{center}
\includegraphics[width=350px]{images/desc.png}
\end{center}
\caption{Boundary descriptors are extracted on a sampled edge, at different
scale. A descriptor is composed of a HoG and an occupancy grid}
\end{figure}

% \caption{The foreground occupancy mask is a grid, where the value of each
% cell represents the ratio of foreground pixels.}

\cite{Arandjelovic11} presents a new shape descriptor (bag of boundaries, BoB), invariant to texture
and color, but also lighting, scale and small viewpoints changes. It needs to
be local, in order to be robust to partial occlusions. \\
The boundary descriptors are formed of two descriptors: a Histogram of
Gradients (HoG) and a foreground mask occupancy grid. The first part is a
4 x 4 HoG cells, containing each 8 x 8 pixels, then L2 normalized. The second
part is 4 x 4 occupancy grid, representing the ratio of foreground pixels in
each cell. It is then L1 normalized, and each element is squared root. The two
descriptors are then concatenated, yielding a 340 element descriptor. \\
Each descriptors is computed at three different scales, 1, 4, and 16 times
1/10 of the foregound object area at interest points. These are computed  on the sampled
edge, in the manner of \cite{BelongieMP02}, using a minimal distance between
each interest points of 25 pixels.

\section{Retrieval procedure}

Traditional object retrieval procedure represents an image with a bag of
visual words, based on SIFT descriptors. Here, we will use a Bag of Boundary
(BoB) representation developed in \cite{Arandjelovic11}. Descriptors are
rendered indexable by quantizing them. The standard method is to use the
Lloyd's k-means. Unfortunately, that forces one to load all the descriptors in
ram. Extracting the descriptors on the training set yields around 800k
descriptors. \cite{fast-k-means} proposes a much faster, online version of
the k-means, enabling us to compute efficiently the vocabulary on the whole
index. \\
Once the vocabulary is build, the inverse index can be easily build, and fits
in ram, enabling fast retrieval from the database. Results are ranked using
tf-idf. To ensure consistency of the results, a spatial verification is done,
using a very loose affine transformation, implemented using RANSAC (figure
\ref{ransac}). \cite{Arandjelovic11} refines the tf idf scoring using:

\begin{equation*}
\score = \tfidf + \alpha n + \beta \frac{n}{n_q} \frac{n}{n_r}
\end{equation*}

Unfortunately, we did not have time to plug in the spatial verification in the
whole toolchain.

\begin{figure}
\label{ransac}
\begin{center}
\includegraphics[width=350px]{images/matching_01.png}
\end{center}
\caption{A very loose affine transformation is fitted, using RANSAC}
\end{figure}

\section{Results}

The sculpture 6k dataset is divided into two: a training set, used for
training the segmentation classifier, and a testing set, for everything else.
To test the smooth object retrieval pipeline, $10$ sculptures of Henry Moore are
chosen, with seven images for these sculptures. This sums up to $70$ queries. \\
For each of these queries, a groundtruth is provided, listing the names of the
\textit{positives} and of \textit{ignores}. The list of \textit{ignores} is
composed of pictures in which less of $25$\% of the sculpture is visible. The
\textit{positives} list for a query only contains the sculpture that match
both the shape (and not the texture) and the view points. That explains why
the groundtruth varies from a query sculpture to another. \\
The retrieval process is then evaluated using the average mean precision (mAP)
over the $70$ queries. The \textit{ignores} are neither counted as positives nor
negatives.

% TODO Add results

\section{Conclusion}

As the mAP shows, results with this new retrieval method are far from being
perfect. Yet, it yields much better results on smooth objects than previous
methods. Hence, boundary descriptors can be considered as good representation
of smooth objects, and shapes. Yet, the whole tool chain (segmentation + BoB)
seems to be slow and memory consuming, mostly due to gPB segmentation part. \\
More work can be done to extend this chain of algorithms to other type of
smooth objects, such as semi transparent objects, but we can expect it to be
also successful. \\


\bibliography{biblio.bib}

\end{document}
